## Pyspark Transformations
In this code, I have performed various operations on Spark dataframes such as converting RDD to dataframe, applying filters and maps, performing unions, and reading data from various file formats including XML, CSV, JSON, and Parquet. I have also used split operations and case statements in my data processing pipeline. Additionally, I created unified column lists and applied them to the dataframes, performed RDD column level processing, and created a schema using StructType and NamedTuple.

## Installation 
* Clone the repository to your local machine using
```sh
https://github.com/mohankrishna02/pyspark-transformations.git
```
* Install PySpark on your machine.
* Import the project into your IDE of choice.
* Provide the correct file paths that are required for the code to run properly.
* Build and run the project.

## Technologies/Tools Used
* PySpark - A fast and general purpose distributed processing frame work. To install the PySpark, you can use the following command
```sh
pip install pyspark==<PySparkVersion>
```
* Python - A high level programming language. You can dwonload the python from here <https://www.python.org/downloads/>
* PyCharm - PyCharm is an integrated development environment used for programming in Python. You can dwonload the PyCharm from here <https://www.jetbrains.com/pycharm/download/#section=windows>

## Documentation
PySpark Documentation - <https://spark.apache.org/docs/latest/>
